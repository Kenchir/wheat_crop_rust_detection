# -*- coding: utf-8 -*-
"""cropDisease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TzmUdSol09YYyOxRaTakGAEJ0KJrV5NF
"""

!pip install tensorflow==2.0

!unzip "drive/My Drive/Projects/cgiar-computer-vision-for-crop-disease.zip";

#Convert all images to jpeg format
!sudo apt install imagemagick
!cd /content/ICLR/test/test; mogrify -format jpeg *.* 
!cd /content/ICLR/train/train/healthy_wheat; mogrify -format jpeg *.* 
!cd /content/ICLR/train/train/leaf_rust; mogrify -format jpeg *.*  
!cd /content/ICLR/train/train/stem_rust; mogrify -format jpeg *.*

from __future__ import absolute_import, division, print_function, unicode_literals
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import tensorflow_hub as hub
from pathlib import Path
import os
from tensorflow.keras import datasets, layers, models
# from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, load_model
from typing import Tuple
from datetime import datetime
from tqdm import tqdm
import re

tf.__version__

data_dir='/content/ICLR'
BATCH_SIZE = 24
IMG_HEIGHT = 224
IMG_WIDTH = 224

#It will automatically adjust the tf.data runtime to tune the value dynamically at runtime for the efficiency issue
AUTOTUNE=tf.data.experimental.AUTOTUNE

test_dir=data_dir+"/test/test"
train_dir=data_dir+"/train/train"

#Delete images that does not end with jpeg
for folder in os.listdir(train_dir):
  for image in os.listdir(train_dir+"/" +folder):
    image_dir=train_dir+"/"+folder+"/"+image
    if image_dir.endswith(".jpeg") ==False :
      os.remove(image_dir)
      
for image in os.listdir(test_dir):
  image_dir=test_dir+"/"+image
  if image_dir.endswith(".jpeg") ==False :
    os.remove(image_dir)

test_count=len(os.listdir(test_dir))
train_count=len(os.listdir(train_dir+"/healthy_wheat"))+len(os.listdir(train_dir+"/leaf_rust"))+len(os.listdir(train_dir+"/stem_rust"))
print("Test image count: ",test_count)
print("Train image count: ",train_count)

STEPS_PER_EPOCH = np.ceil(train_count/BATCH_SIZE)
train_ds = tf.data.Dataset.list_files(str(train_dir+'/*/*'))

# #Function to split data to train and val
# def split_dataset(dataset: tf.data.Dataset, 
#                   dataset_size: int, 
#                   train_ratio:float,
#                   validation_ratio: float) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
#     assert (train_ratio + validation_ratio) <= 1

#     train_count = int(dataset_size * train_ratio)
#     validation_count = int(dataset_size * validation_ratio)

#     dataset = dataset.shuffle(dataset_size)

#     train_dataset = dataset.take(train_count)
#     validation_dataset = dataset.skip(train_count).take(validation_count)
#     return train_dataset, validation_dataset

# size_of_ds = 876
# train_ratio = 0.9
# val_ratio = 0.1


# train_ds, val_ds = split_dataset(train_files, size_of_ds, train_ratio, val_ratio)

CLASS_NAMES = np.array([item.name for item in Path(train_dir).glob('*')])
CLASS_NAMES

def get_label(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  return parts[-2] == CLASS_NAMES

def decode_img(img):
  # convert the compressed string to a 3D uint8 tensor
  img = tf.image.decode_jpeg(img, channels=3)
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  img = tf.image.convert_image_dtype(img, tf.float32)
  # resize the image to the desired size.
  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])/255.0

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
train_labeled_ds = train_ds.map(process_path, num_parallel_calls=1)

for image, label in train_labeled_ds.take(1):
  print("Image shape: ", image.numpy().shape)
  print("Label: ", label.numpy())

def prepare_for_training(ds, shuffle_buffer_size=10, training=True):
  ds = ds.shuffle(buffer_size=shuffle_buffer_size)

  # Repeat forever
  if training:
    ds = ds.repeat()
    print("repeating")
  else:
    pass
  ds = ds.batch(BATCH_SIZE)

  # `prefetch` lets the dataset fetch batches in the background while the model
  # is training.
  ds = ds.prefetch(buffer_size=-1)

  return ds

train_ds_ = prepare_for_training(train_labeled_ds)

image_batch,label_batch=next(iter(train_ds_))

image_batch.shape

def show_batch(image_batch, label_batch):
  plt.figure(figsize=(10,10))
  for n in range(10):
      ax = plt.subplot(5,5,n+1)
      plt.imshow(image_batch[n])
      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())
      plt.axis('off')

show_batch(image_batch.numpy(), label_batch.numpy())

test_list_ds = tf.data.Dataset.list_files(str(test_dir +'*/*'))

for f in test_list_ds.take(5):
  print(f.numpy())

def process_test(image_path):
    img = decode_img(tf.io.read_file(image_path))
    return img,image_path
test_ds = test_list_ds.map(process_test, num_parallel_calls=-1)

steps_per_epoch = 876 // BATCH_SIZE

print(steps_per_epoch)

feature_extractor_url = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4" #@param {type:"string"}

feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
feature_extractor_layer.trainable = False

model = tf.keras.Sequential([
  feature_extractor_layer,
  layers.Dense(3, activation='softmax')
])

model.summary()

model.compile(
  optimizer=tf.keras.optimizers.Adam(),
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

class CollectBatchStats(tf.keras.callbacks.Callback):
  def __init__(self):
    self.batch_losses = []
    self.batch_acc = []

  def on_train_batch_end(self, batch, logs=None):
    self.batch_losses.append(logs['loss'])
    self.batch_acc.append(logs['accuracy'])
    self.model.reset_metrics()

steps_per_epoch = STEPS_PER_EPOCH

batch_stats_callback = CollectBatchStats()

history = model.fit_generator(train_ds_, epochs=8,
                              steps_per_epoch=steps_per_epoch,
                             
                              callbacks = [batch_stats_callback])

# IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)

# # Create the base model from the pre-trained model MobileNet V2
# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
#                                                include_top=False,
#                                                weights='imagenet')

# base_model.trainable = False
# # Let's take a look at the base model architecture
# base_model.summary()

# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
# prediction_layer = tf.keras.layers.Dense(3,activation='softmax')

# model = tf.keras.Sequential([
#   base_model,
#   global_average_layer,
#   prediction_layer
# ])

# base_learning_rate = 0.0001
# model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
#               metrics=['accuracy'])

# history = model.fit(train_ds_,
#                     epochs=5,
#                     steps_per_epoch = STEPS_PER_EPOCH
#                     )

id_names = []
predictions = []

for i, j in tqdm(test_ds):
    i = i.numpy()[np.newaxis, :] # add a new dimension
    prediction = model.predict_proba(i) # make predictions
    predictions.append(prediction) 
    
    # use regular expressions to extract the name of image
    name = j.numpy()
    name = re.sub("[^A-Z0-9]", "", str(name))
    name = name.replace("JPEG", "")
    name = name.replace("jpeg", "")
    name=name.replace(name[:4], '')
    id_names.append(name)

leaf_rust = pd.Series(range(610), name="leaf_rust", dtype=np.float32)
stem_rust = pd.Series(range(610), name="stem_rust", dtype=np.float32)
healthy_wheat = pd.Series(range(610), name="healthy_wheat", dtype=np.float32)

sub = pd.concat([leaf_rust, stem_rust, healthy_wheat], axis=1)

sub.shape

# append real predictions to the dataset
for i in tqdm(range(0 ,len(predictions))):
    sub.loc[i] = predictions[i]

sub.tail()

sub["ID"] = id_names

cols = sub.columns.tolist()
cols = cols[-1:] + cols[:-1]
sub = sub[cols]

sub.to_csv("submission.csv", index=False)

df=pd.read_csv("submission.csv")

df.info